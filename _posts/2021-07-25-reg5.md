---
title: "다항회귀"
date: 2021-07-25

categories:
 - Regression

tags:
 - [데이터 청년 캠퍼스, Regression]

toc: true
---
```py
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
```  

## sample로 연습  
```py
X = np.arange(4).reshape(2,2)

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit(X)  # 변환기의 fit은 별로 하는 일이 없음
X_poly.transform(X)
```  
<div class="notice--primary" markdown="1">
`reshape`: 행, 열을 나눠줌 → `.reshape(행, 열)`  
`reshape(-1, 1)`: -1을 신경쓰지 말고 나머지 먼저 맞춰주고 채워줌  
`fit`: 변환기 → 독립변수만 변환  
변환기는 입력변수만 넣음  
`fit`, `transform`을 나눠 적어도 되고 `fit_transform`으로 합쳐 적어도 됨  
</div>

- 어떤 항(특성)이 곱해지는 지(만들어지는 지) 알고 싶을 때  
  ```py
  poly.get_feature_names()
  ```  
  
  |**1**|x0|x1|x0^2|x0 * x1|x0^1|  
  |:----:|:----:|:----:|:----:|:----:|:----:|  
  |1 |0 | 1|0 |0 |1 |  
  |1 |2 |3 |4 |6 |9 |  

  
## 사이킷런으로 다항 회귀 분석  
```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
```  

### 데이터 준비: 데이터를 생성하여 탐색 및 확인  
```py
X = 6*np.random.rand(100,1) - 3
y = 0.5*X**2 + X + 2 + np.random.randn(100,1)

plt.scatter(X, y, color='blue')
plt.show()
```  

<div class="notice--primary" markdown="1">
np.random.rand(100,1): 0부터 1사이의 난수를 100개  
np.random.randn(100,1): 0부터 1사이의 정규분포가 되도록 난수를 100개  
</div>  

### 단순 선형 회귀 분석 및 시각화: 비교 분석  
```py
# 1차항
X1_train = X
y_train = y
reg1 = LinearRegression().fit(X1_train, y_train)

xx = np.arange(-3, 3, 0.01)[:, np.newaxis]
yy = reg1.predict(xx)

plt.plot(xx, yy, color='r', linestyle='--', label='degree=1')
plt.scatter(X, y, color='b')
plt.legend()
plt.show()
```  

<div class="notice--primary" markdown="1">  
`np.arange(-3, 3, 0.01)`: -3부터 3까지 0.01씩 증가  
`[:, np.newaxis]`: 차원 증가, `:`: 전체 범위 적용, `newaxis`: 축을 하나 더 세움  
</div>  

- shape
  ```py
  X1_train.shape
  ```  
  <div class="notice" markdown="1">  
  (100, 1)  
  </div>  

### 2차항 형태로 다항 회귀 수행  
```py
poly = PolynomialFeatures(degree=2)

X2_train = poly.fit_transform(X)
y_train = y
reg2 = LinearRegression().fit(X2_train, y_train)

xx = np.arange(-3, 3, 0.01)
yy = reg2.predict(poly.transform(xx[:, np.newaxis]))

plt.plot(xx, yy, color='g', linestyle='--', label='degree=2')
plt.scatter(X, y, color='b')  # 원본
plt.legend()
plt.show()
```  

<div class="notice--primary" markdown="1">  
`PolynomialFeatures`: 변환기  
변환기를 만들면 그 변환기의 기준에 다른 입력 데이더를 넣어 줘야함  
처음 변환을 할때는 `fit_transform`, 그 변환기에 넣을 때는 `transform`  
</div>  

- 구해진 2차식의 계수 확인  
  ```py
  reg2.intercept_, reg2.coef_
  ```
  <div class="notice--primary" markdown="1">  
  poly = PolynomialFeatures(degree=2) 로 만듦 -> reg2.coef_하면 상수항도 같이 뜸  
  </div>  
  <div class="notice" markdown="1">
  (array([1.88968758]), array([[0.        , 0.92543067, 0.51905856]]))  
  </div>  
  
  |**상수**|x|x^2|  
  |:----:|:----:|:----:|  
  |0 |0.92543067 |0.51905856 |  
    
- shape  
  ```py
  X2_train.shape
  ```  
  <div class="notice" markdown="1">
  (100, 3)  
  </div>
  
### 7차항 형태의 다항회귀  
```py
poly = PolynomialFeatures(degree=7)

X7_train = poly.fit_transform(X)
y_train = y
reg7 = LinearRegression().fit(X7_train, y_train)

xx = np.arange(-3, 3, 0.01)
yy = reg7.predict(poly.transform(xx[:, np.newaxis]))

plt.plot(xx, yy, color='m', linestyle='--', label='degree=7')
plt.scatter(X, y, color='b')  # 원본
plt.legend()
plt.show()
```  

- shape  
  ```py
  X7_train.shape
  ```  
  <div class="notice" markdown="1">
  (100, 8)  
  </div>
  
### 여러 형태의 시각화 그래프 비교  
```py
degree = [1, 2, 7]
colors = ['red', 'green', 'magenta']

plt.scatter(X, y, color='b', alpha=0.2)
for i, d in enumerate(degree):
    poly = PolynomialFeatures(degree = d)
    X_train = poly.fit_transform(X)
    y_train = y
    
    reg = LinearRegression().fit(X_train, y_train)
    xx = np.arange(-3, 3, 0.01)
    yy = reg.predict(poly.transform(xx[:, np.newaxis]))
    
    plt.plot(xx, yy, color=colors[i], linestyle='--', label='degree={}'.format(d))
    plt.legend()
plt.show()
```  

<div class="notice--primary" markdown="1">
`enumerate`: list에서 인덱스 번호를 가져옴  
`degree={}.format(d)`: format(d)의 내용이 {}안에 들어감  
  
X ---(변환기:fit, transform)---------> transform ------(추정기: predict)--------> y  
pipeline: 여러 단계를 거치는 것을 줄이고 데이터 손실도 없애기 위한 전처리 모듈  
          데이터 흐름을 하나로 묶어서 준다  
</div>  

## pipelline을 활용한 다항 회귀  
```py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

X = 6*np.random.rand(100,1) - 3
y = 0.5*X**2 + X + 2 + np.random.randn(100,1)
```

### degree=[1,2,7]인 모델  
```py
degree = [1, 2, 7]
colors = ['red', 'green', 'magenta']

plt.scatter(X, y, color='b', alpha=0.2)

for i, d in enumerate(degree):
    reg = make_pipeline(PolynomialFeatures(degree=2), LinearRegression()).fit(X, y)

    xx = np.arange(-3, 3, 0.01)[:, np.newaxis]
    yy = reg.predict(xx)

    plt.plot(xx, yy, color=colors[i], linestyle='--', label='degree={}'.format(d))
    plt.legend()
plt.show()
```  
<div class="notice--primary" markdown="1">  
`pipeline`을 하면 제일 마지막에 들어간 추정기의 이름인 객체가 만들어진다 → LinearRegression객체가 만들어짐  
</div>  
